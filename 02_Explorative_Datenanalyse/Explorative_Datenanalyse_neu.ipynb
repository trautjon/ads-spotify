{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 1: Daten in die SQL-Datenbank importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erforderliche Pakete installieren\n",
    "pip install pandas sqlalchemy pymysql openpyxl seaborn\n",
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Float\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Sensible Daten aus Umgebungsvariablen lesen\n",
    "db_user = os.getenv('DB_USER', 'root')\n",
    "db_password = os.getenv('DB_PASSWORD', 'example')\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_name = os.getenv('DB_NAME', 'spotify_data')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "# Ordnerpfad mit Excel-Dateien (entweder als rohe Zeichenfolge oder mit doppelten Backslashes)\n",
    "folder_path = r'/Users/bavaarde/ads-spotify/2024-05-18'\n",
    "# oder\n",
    "# folder_path = 'C:\\\\applied_data_science\\\\gruppen_projekt\\\\ads-spotify\\\\ads-spotify\\\\2024-04-27'\n",
    "\n",
    "# Funktion, um SQLAlchemy-Datentypen basierend auf Pandas-Datentypen zu bestimmen\n",
    "def map_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return Integer()\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return Float()\n",
    "    else:\n",
    "        return String(255)  # Standard-Stringlänge 255\n",
    "\n",
    "# Alle Excel-Dateien im Ordner durchgehen\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Excel-Datei in ein DataFrame laden\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Tabellenname basierend auf dem Dateinamen (ohne Erweiterung)\n",
    "        table_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Metadaten und Tabelle definieren\n",
    "        metadata = MetaData()\n",
    "        columns = []\n",
    "        for col_name, dtype in zip(df.columns, df.dtypes):\n",
    "            col_type = map_dtype(dtype)\n",
    "            columns.append(Column(col_name, col_type))\n",
    "        \n",
    "        table = Table(table_name, metadata, *columns)\n",
    "        \n",
    "        try:\n",
    "            # Tabelle in der Datenbank erstellen\n",
    "            metadata.create_all(engine)\n",
    "            \n",
    "            # Daten in die Tabelle einfügen\n",
    "            df.to_sql(table_name, engine, index=False, if_exists='append')\n",
    "            print(f\"Tabelle '{table_name}' erfolgreich erstellt und Daten importiert.\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Fehler beim Erstellen der Tabelle '{table_name}': {e}\")\n",
    "\n",
    "print(\"Vorgang abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2: Explorative Datenanalyse (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA_Script.ipynb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Sensible Daten aus Umgebungsvariablen lesen\n",
    "db_user = os.getenv('DB_USER', 'root')\n",
    "db_password = os.getenv('DB_PASSWORD', 'example')\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_name = os.getenv('DB_NAME', 'spotify_data')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "# Inspector verwenden, um Tabelleninformationen abzurufen\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Alle Tabellennamen abrufen\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Daten aus jeder Tabelle laden und analysieren\n",
    "for table_name in table_names:\n",
    "    print(f\"\\nAnalysiere Tabelle: {table_name}\")\n",
    "    query = f\"SELECT * FROM `{table_name}`\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    print(f\"Erster Blick auf die Daten in Tabelle '{table_name}':\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"\\nStatistische Zusammenfassung der numerischen Daten:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    print(\"\\nInformationen über das DataFrame:\")\n",
    "    print(df.info())\n",
    "\n",
    "    # Fehlende Werte analysieren\n",
    "    print(\"\\nAnzahl fehlender Werte pro Spalte:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Verteilung von numerischen Variablen\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    print(\"\\nVerteilung der numerischen Variablen:\")\n",
    "    df[numerical_columns].hist(bins=15, figsize=(15, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Korrelationen zwischen numerischen Variablen\n",
    "    print(\"\\nKorrelationsmatrix der numerischen Variablen:\")\n",
    "    corr_matrix = df[numerical_columns].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.title(f\"Korrelationsmatrix der numerischen Variablen in Tabelle '{table_name}'\")\n",
    "    plt.show()\n",
    "\n",
    "    # Verteilung von kategorialen Variablen\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(\"\\nVerteilung der kategorialen Variablen:\")\n",
    "    for column in categorical_columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.countplot(data=df, x=column, palette='Set2')\n",
    "        plt.title(f\"Verteilung der Kategorie: {column} in Tabelle '{table_name}'\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "    # Paarweise Verteilung von ausgewählten Variablen\n",
    "    print(\"\\nPaarweise Verteilung von ausgewählten Variablen:\")\n",
    "    selected_columns = numerical_columns[:5]  # Wählen Sie bis zu 5 Spalten für den Paarplot\n",
    "    if len(selected_columns) > 1:  # Sicherstellen, dass genug Spalten für den Pairplot vorhanden sind\n",
    "        sns.pairplot(df[selected_columns])\n",
    "        plt.show()\n",
    "\n",
    "    # Boxplots zur Analyse von Ausreissern\n",
    "    print(\"\\nBoxplots zur Analyse von Ausreissern:\")\n",
    "    for column in numerical_columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.boxplot(data=df, x=column, palette='Set2')\n",
    "        plt.title(f\"Boxplot der Variable: {column} in Tabelle '{table_name}'\")\n",
    "        plt.show()\n",
    "\n",
    "    # Analyse von Beziehungen zwischen kategorialen und numerischen Variablen\n",
    "    print(\"\\nAnalyse von Beziehungen zwischen kategorialen und numerischen Variablen:\")\n",
    "    for cat_col in categorical_columns:\n",
    "        for num_col in numerical_columns:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.boxplot(data=df, x=cat_col, y=num_col, palette='Set2')\n",
    "            plt.title(f\"Beziehung zwischen {cat_col} und {num_col} in Tabelle '{table_name}'\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "    # Zeitanalyse, falls Datumsspalte vorhanden ist\n",
    "    date_columns = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "\n",
    "    print(\"\\nZeitanalyse, falls Datumsspalte vorhanden ist:\")\n",
    "    for date_col in date_columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])  # Konvertieren zu datetime\n",
    "        df.set_index(date_col, inplace=True)\n",
    "        df.resample('M').mean().plot(figsize=(15, 5))\n",
    "        plt.title(f\"Zeitanalyse der Spalte: {date_col} in Tabelle '{table_name}'\")\n",
    "        plt.show()\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "print(\"\\nExplorative Datenanalyse abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 3: Machine Learning Modelle erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_Script.ipynb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix\n",
    "\n",
    "# Sensible Daten aus Umgebungsvariablen lesen\n",
    "db_user = os.getenv('DB_USER', 'root')\n",
    "db_password = os.getenv('DB_PASSWORD', 'example')\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_name = os.getenv('DB_NAME', 'spotify_data')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "# Inspector verwenden, um Tabelleninformationen abzurufen\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Alle Tabellennamen abrufen\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - Mean Squared Error: {mse}\")\n",
    "    print(f\"{model_name} - R² Score: {r2}\")\n",
    "\n",
    "# Daten aus jeder Tabelle laden und analysieren\n",
    "for table_name in table_names:\n",
    "    print(f\"\\nAnalysiere Tabelle: {table_name}\")\n",
    "    query = f\"SELECT * FROM `{table_name}`\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Überprüfen, ob genug Daten vorhanden sind\n",
    "    if df.shape[1] < 2:\n",
    "        print(f\"Nicht genug Daten in Tabelle '{table_name}' für ML-Analyse.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Erster Blick auf die Daten in Tabelle '{table_name}':\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Beispiel: Feature und Target definieren\n",
    "    # Annahme: Die letzte Spalte ist das Ziel (Target) und die restlichen Spalten sind Features\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    # Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Numerische und kategoriale Spalten identifizieren\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Preprocessing Pipelines für numerische und kategoriale Daten\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Modellauswahl und -training\n",
    "\n",
    "    # Linear Regression\n",
    "    print(\"Training des linearen Regressionsmodells...\")\n",
    "    lr_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "    # Random Forest Regressor\n",
    "    print(\"Training des Random Forest Regressors...\")\n",
    "    rf_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Hyperparameter-Tuning mit GridSearchCV\n",
    "    param_grid = {\n",
    "        'regressor__n_estimators': [100, 200],\n",
    "        'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'regressor__max_depth': [10, 20, None]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "    # Modellbewertung\n",
    "    print(\"Modellbewertung...\")\n",
    "    \n",
    "    # Bewertung der Modelle\n",
    "    evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "    evaluate_model(y_test, y_pred_rf, \"Random Forest Regressor\")\n",
    "\n",
    "    # Visualisierung der Ergebnisse\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(y_test, y_pred_lr, alpha=0.5, label='Linear Regression')\n",
    "    plt.scatter(y_test, y_pred_rf, alpha=0.5, label='Random Forest Regressor')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.legend()\n",
    "    plt.title(f'True vs Predicted Values für Tabelle {table_name}')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Machine Learning Analyse abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 4: Geographische Datenintegration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erforderliche Pakete installieren:\n",
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographische Daten:\n",
      "   id    name                                           geometry\n",
      "0   1  Unit 1  POLYGON ((-99.00000 41.00000, -99.00000 42.000...\n",
      "1   2  Unit 2  POLYGON ((-100.00000 43.00000, -100.00000 44.0...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Argentinien 2024-05-18\n",
      "Tabelle 'Top 50 – Argentinien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Australien 2024-05-18\n",
      "Tabelle 'Top 50 – Australien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Belarus 2024-05-18\n",
      "Tabelle 'Top 50 – Belarus 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Belgien 2024-05-18\n",
      "Tabelle 'Top 50 – Belgien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Bolivien 2024-05-18\n",
      "Tabelle 'Top 50 – Bolivien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Brasilien 2024-05-18\n",
      "Tabelle 'Top 50 – Brasilien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Bulgarien 2024-05-18\n",
      "Tabelle 'Top 50 – Bulgarien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Chile 2024-05-18\n",
      "Tabelle 'Top 50 – Chile 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Costa Rica 2024-05-18\n",
      "Tabelle 'Top 50 – Costa Rica 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Deutschland 2024-05-18\n",
      "Tabelle 'Top 50 – Deutschland 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Dominikanische Republik 2024-05-18\n",
      "Tabelle 'Top 50 – Dominikanische Republik 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Dänemark 2024-05-18\n",
      "Tabelle 'Top 50 – Dänemark 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Ecuador 2024-05-18\n",
      "Tabelle 'Top 50 – Ecuador 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – El Salvador 2024-05-18\n",
      "Tabelle 'Top 50 – El Salvador 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Estland 2024-05-18\n",
      "Tabelle 'Top 50 – Estland 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Finnland 2024-05-18\n",
      "Tabelle 'Top 50 – Finnland 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Frankreich 2024-05-18\n",
      "Tabelle 'Top 50 – Frankreich 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Global 2024-05-18\n",
      "Tabelle 'Top 50 – Global 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Griechenland 2024-05-18\n",
      "Tabelle 'Top 50 – Griechenland 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Großbritannien 2024-05-18\n",
      "Tabelle 'Top 50 – Großbritannien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Guatemala 2024-05-18\n",
      "Tabelle 'Top 50 – Guatemala 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Honduras 2024-05-18\n",
      "Tabelle 'Top 50 – Honduras 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Hong Kong 2024-05-18\n",
      "Tabelle 'Top 50 – Hong Kong 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Indien 2024-05-18\n",
      "Tabelle 'Top 50 – Indien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Indonesien 2024-05-18\n",
      "Tabelle 'Top 50 – Indonesien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Irland 2024-05-18\n",
      "Tabelle 'Top 50 – Irland 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Island 2024-05-18\n",
      "Tabelle 'Top 50 – Island 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Israel 2024-05-18\n",
      "Tabelle 'Top 50 – Israel 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Italien 2024-05-18\n",
      "Tabelle 'Top 50 – Italien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Japan 2024-05-18\n",
      "Tabelle 'Top 50 – Japan 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Kanada 2024-05-18\n",
      "Tabelle 'Top 50 – Kanada 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Kasachstan 2024-05-18\n",
      "Tabelle 'Top 50 – Kasachstan 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Kolumbien 2024-05-18\n",
      "Tabelle 'Top 50 – Kolumbien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Lettland 2024-05-18\n",
      "Tabelle 'Top 50 – Lettland 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Litauen 2024-05-18\n",
      "Tabelle 'Top 50 – Litauen 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Luxemburg 2024-05-18\n",
      "Tabelle 'Top 50 – Luxemburg 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Malaysia 2024-05-18\n",
      "Tabelle 'Top 50 – Malaysia 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Marokko 2024-05-18\n",
      "Tabelle 'Top 50 – Marokko 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Mexiko 2024-05-18\n",
      "Tabelle 'Top 50 – Mexiko 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Neuseeland 2024-05-18\n",
      "Tabelle 'Top 50 – Neuseeland 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Nicaragua 2024-05-18\n",
      "Tabelle 'Top 50 – Nicaragua 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Niederlande 2024-05-18\n",
      "Tabelle 'Top 50 – Niederlande 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Nigeria 2024-05-18\n",
      "Tabelle 'Top 50 – Nigeria 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Norwegen 2024-05-18\n",
      "Tabelle 'Top 50 – Norwegen 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Pakistan 2024-05-18\n",
      "Tabelle 'Top 50 – Pakistan 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Panama 2024-05-18\n",
      "Tabelle 'Top 50 – Panama 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Paraguay 2024-05-18\n",
      "Tabelle 'Top 50 – Paraguay 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Peru 2024-05-18\n",
      "Tabelle 'Top 50 – Peru 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Philippinen 2024-05-18\n",
      "Tabelle 'Top 50 – Philippinen 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Polen 2024-05-18\n",
      "Tabelle 'Top 50 – Polen 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Portugal 2024-05-18\n",
      "Tabelle 'Top 50 – Portugal 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Rumänien 2024-05-18\n",
      "Tabelle 'Top 50 – Rumänien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Saudi-Arabien 2024-05-18\n",
      "Tabelle 'Top 50 – Saudi-Arabien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Schweden 2024-05-18\n",
      "Tabelle 'Top 50 – Schweden 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Schweiz 2024-05-18\n",
      "Tabelle 'Top 50 – Schweiz 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Singapur 2024-05-18\n",
      "Tabelle 'Top 50 – Singapur 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Slowakei 2024-05-18\n",
      "Tabelle 'Top 50 – Slowakei 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Spanien 2024-05-18\n",
      "Tabelle 'Top 50 – Spanien 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Südafrika 2024-05-18\n",
      "Tabelle 'Top 50 – Südafrika 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Südkorea 2024-05-18\n",
      "Tabelle 'Top 50 – Südkorea 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Taiwan 2024-05-18\n",
      "Tabelle 'Top 50 – Taiwan 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Thailand 2024-05-18\n",
      "Tabelle 'Top 50 – Thailand 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Tschechische Republik 2024-05-18\n",
      "Tabelle 'Top 50 – Tschechische Republik 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Türkei 2024-05-18\n",
      "Tabelle 'Top 50 – Türkei 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – USA 2024-05-18\n",
      "Tabelle 'Top 50 – USA 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Ukraine 2024-05-18\n",
      "Tabelle 'Top 50 – Ukraine 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Ungarn 2024-05-18\n",
      "Tabelle 'Top 50 – Ungarn 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Uruguay 2024-05-18\n",
      "Tabelle 'Top 50 – Uruguay 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Venezuela 2024-05-18\n",
      "Tabelle 'Top 50 – Venezuela 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Vereinigte Arabische Emirate 2024-05-18\n",
      "Tabelle 'Top 50 – Vereinigte Arabische Emirate 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Vietnam 2024-05-18\n",
      "Tabelle 'Top 50 – Vietnam 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Ägypten 2024-05-18\n",
      "Tabelle 'Top 50 – Ägypten 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "\n",
      "Analysiere Tabelle: Top 50 – Österreich 2024-05-18\n",
      "Tabelle 'Top 50 – Österreich 2024-05-18' enthält keine 'location_id' Spalte. Überspringen...\n",
      "Geographische Datenanalyse abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Sensible Daten aus Umgebungsvariablen lesen\n",
    "db_user = os.getenv('DB_USER', 'root')\n",
    "db_password = os.getenv('DB_PASSWORD', 'example')\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_name = os.getenv('DB_NAME', 'spotify_data')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "# Inspector verwenden, um Tabelleninformationen abzurufen\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Alle Tabellennamen abrufen\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Beispielhafte geographische Daten laden\n",
    "geojson_path = r'/Users/bavaarde/ads-spotify/02_Explorative_Datenanalyse/sample.geojson'  # Ersetzen Sie den Pfad durch den tatsächlichen Pfad\n",
    "\n",
    "if os.path.exists(geojson_path):\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    print(\"Geographische Daten:\")\n",
    "    print(gdf.head())\n",
    "else:\n",
    "    print(f\"Datei nicht gefunden: {geojson_path}\")\n",
    "    raise FileNotFoundError(f\"Die Datei unter dem Pfad {geojson_path} wurde nicht gefunden.\")\n",
    "\n",
    "# Daten aus jeder Tabelle laden und analysieren\n",
    "for table_name in table_names:\n",
    "    print(f\"\\nAnalysiere Tabelle: {table_name}\")\n",
    "    query = f\"SELECT * FROM `{table_name}`\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Überprüfen, ob die Tabelle die erwarteten Spalten enthält\n",
    "    if 'location_id' not in df.columns:\n",
    "        print(f\"Tabelle '{table_name}' enthält keine 'location_id' Spalte. Überspringen...\")\n",
    "        continue\n",
    "    \n",
    "    # Daten zusammenführen\n",
    "    # Angenommen, df hat eine Spalte 'location_id', die mit einer Spalte 'id' in gdf übereinstimmt\n",
    "    merged = gdf.merge(df, left_on='id', right_on='location_id')\n",
    "    \n",
    "    print(f\"Erster Blick auf die Daten in Tabelle '{table_name}':\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Datenvisualisierung\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    gdf.plot(ax=ax, color='blue', edgecolor='black')\n",
    "    plt.title(f\"Geographische Datenvisualisierung für Tabelle '{table_name}'\")\n",
    "    plt.show()\n",
    "\n",
    "    # Detaillierte Visualisierung mit Daten\n",
    "    # Beispiel: Visualisierung der Datenverteilung auf der Karte\n",
    "    if 'data_column' in merged.columns:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        merged.plot(column='data_column', ax=ax, legend=True,\n",
    "                    legend_kwds={'label': \"Data Value\",\n",
    "                                 'orientation': \"horizontal\"})\n",
    "        plt.title(f\"Geographische Daten mit Attributen für Tabelle '{table_name}'\")\n",
    "        plt.show()\n",
    "\n",
    "        # Weitere Analysen\n",
    "        # Beispiel: Durchschnittswert pro geographische Einheit berechnen\n",
    "        average_values = merged.groupby('geo_column')['data_column'].mean().reset_index()\n",
    "        print(f\"Durchschnittswerte pro geographische Einheit in Tabelle '{table_name}':\")\n",
    "        print(average_values)\n",
    "\n",
    "        # Visualisierung der Durchschnittswerte auf der Karte\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        merged_avg = gdf.merge(average_values, left_on='id', right_on='geo_column')\n",
    "        merged_avg.plot(column='data_column', ax=ax, legend=True,\n",
    "                        legend_kwds={'label': \"Average Value\",\n",
    "                                     'orientation': \"horizontal\"})\n",
    "        plt.title(f\"Durchschnittswerte pro geographische Einheit in Tabelle '{table_name}'\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Tabelle '{table_name}' enthält keine 'data_column' Spalte für die Visualisierung.\")\n",
    "        \n",
    "print(\"Geographische Datenanalyse abgeschlossen.\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 5: Fortgeschrittene Deep Learning Techniken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erforderliches Pakete installieren:\n",
    "pip install tensorflow pandas sqlalchemy pymysql nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysiere Tabelle: Top 50 – Argentinien 2024-05-18\n",
      "Erster Blick auf die Daten in Tabelle 'Top 50 – Argentinien 2024-05-18':\n",
      "   Unnamed: 0                Track_ID  Artist_Name  \\\n",
      "0           0  3GD6eImRvT0zgr8cQnokUq        Bhavi   \n",
      "1           1  5rQSQlZXXjMcevPGoAfE1z  Salastkbron   \n",
      "2           2  4wS0TnQzVkY9ML1BPKpOk1    Tiago PZK   \n",
      "3           3  7bywjHOc0wSjGGbj04XbVi         Feid   \n",
      "4           4  6XjDF6nds4DE2BBbagZol6   FloyyMenor   \n",
      "\n",
      "                                          Track_Name  Popularity Explicit  \\\n",
      "0  BESAME (feat. Tiago PZK, Khea & Neo Pistea) - ...          86        1   \n",
      "1                                      Un Besito Más          79        0   \n",
      "2                                               Piel          86        1   \n",
      "3                                               LUNA          93        0   \n",
      "4                                          Gata Only          98        1   \n",
      "\n",
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.728   0.618    9    -6.621     1       0.0673         0.203   \n",
      "1         0.693   0.641    2    -5.775     0       0.0359         0.368   \n",
      "2         0.646   0.907    0    -1.617     1       0.0593         0.422   \n",
      "3         0.774   0.860    7    -2.888     0       0.1300         0.131   \n",
      "4         0.791   0.499    8    -8.472     0       0.0509         0.446   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  \n",
      "0          0.000000    0.1270    0.217  130.104  \n",
      "1          0.000000    0.2940    0.514   92.010  \n",
      "2          0.000000    0.1400    0.816  168.004  \n",
      "3          0.000000    0.1160    0.446  100.019  \n",
      "4          0.000024    0.0899    0.669   99.986  \n",
      "\n",
      "\n",
      "Datenvorverarbeitung...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bavaarde/nltk_data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '0mJRX0WnqWlu8Boe7gpQ1P'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d8/gs1syxf55h36dfnt2jysrf680000gn/T/ipykernel_60291/246105159.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Skalierung der Daten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Modell erstellen - MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             return_tuple = (\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \"\"\"\n\u001b[1;32m    874\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1471\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \"\"\"\n\u001b[1;32m    911\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    913\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    994\u001b[0m                         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m                 raise ValueError(\n\u001b[1;32m   1000\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     def __array__(\n\u001b[1;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '0mJRX0WnqWlu8Boe7gpQ1P'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Sicherstellen, dass die NLTK-Ressourcen heruntergeladen sind\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Sensible Daten aus Umgebungsvariablen lesen\n",
    "db_user = os.getenv('DB_USER', 'root')\n",
    "db_password = os.getenv('DB_PASSWORD', 'example')\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_name = os.getenv('DB_NAME', 'spotify_data')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "# Inspector verwenden, um Tabelleninformationen abzurufen\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Alle Tabellennamen abrufen\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Funktion zur Bewertung und Visualisierung des Modells\n",
    "def evaluate_and_visualize_model(model, X_test_scaled, y_test, history, model_name):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} - Mean Squared Error: {mse}\")\n",
    "    print(f\"{model_name} - R² Score: {r2}\")\n",
    "\n",
    "    # Trainings- und Validierungsverluste visualisieren\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'Training and Validation Loss - {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualisierung der tatsächlichen vs. vorhergesagten Werte\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'True vs Predicted Values - {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Durchlaufen aller Tabellen\n",
    "for table_name in table_names:\n",
    "    print(f\"\\nAnalysiere Tabelle: {table_name}\")\n",
    "    query = f\"SELECT * FROM `{table_name}`\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    if df.shape[1] < 2:\n",
    "        print(f\"Nicht genug Daten in Tabelle '{table_name}' für ML-Analyse.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Erster Blick auf die Daten in Tabelle '{table_name}':\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Datenvorverarbeitung\n",
    "    print(\"Datenvorverarbeitung...\")\n",
    "\n",
    "    # Annahme: Die letzte Spalte ist das Ziel (Target) und die restlichen Spalten sind Features\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    # Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Skalierung der Daten\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Modell erstellen - MLP\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Regression - keine Aktivierungsfunktion in der Ausgangsschicht\n",
    "    ])\n",
    "\n",
    "    # Modell kompilieren\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "    # Modelltraining mit Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Modellbewertung und Visualisierung - MLP\n",
    "    print(\"Modellbewertung - MLP...\")\n",
    "    evaluate_and_visualize_model(model, X_test_scaled, y_test, history, \"MLP\")\n",
    "\n",
    "    # Ergänzung: Verwendung eines Convolutional Neural Networks (CNN)\n",
    "\n",
    "    # Reshape der Daten für CNN\n",
    "    X_train_cnn = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1, 1)\n",
    "    X_test_cnn = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1, 1)\n",
    "\n",
    "    # Modell erstellen - CNN\n",
    "    cnn_model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(X_train_cnn.shape[1], 1, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Regression - keine Aktivierungsfunktion in der Ausgangsschicht\n",
    "    ])\n",
    "\n",
    "    # Modell kompilieren - CNN\n",
    "    cnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "    # Modelltraining mit Early Stopping - CNN\n",
    "    history_cnn = cnn_model.fit(\n",
    "        X_train_cnn, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Modellbewertung und Visualisierung - CNN\n",
    "    print(\"Modellbewertung - CNN...\")\n",
    "    evaluate_and_visualize_model(cnn_model, X_test_cnn, y_test, history_cnn, \"CNN\")\n",
    "\n",
    "    # Ergänzung: Verwendung von NLP-Techniken (z.B. Sentiment Analysis)\n",
    "\n",
    "    # Sentiment Analysis mit VADER\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Optional: Sentiment Analysis auf Textdaten in der Datenbank anwenden\n",
    "    # Angenommen, es gibt eine Spalte 'TextColumn' in der Datenbank\n",
    "    if 'TextColumn' in df.columns:\n",
    "        df['sentiment'] = df['TextColumn'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "        print(\"Sentiment Analysis auf Textdaten angewendet.\")\n",
    "        print(df[['TextColumn', 'sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 6: Modellinterpretation und Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erforderliche Pakete installieren:\n",
    "pip install tensorflow pandas sqlalchemy pymysql scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysiere Tabelle: Top 50 – Argentinien 2024-05-18\n",
      "Erster Blick auf die Daten in Tabelle 'Top 50 – Argentinien 2024-05-18':\n",
      "   Unnamed: 0                Track_ID  Artist_Name  \\\n",
      "0           0  3GD6eImRvT0zgr8cQnokUq        Bhavi   \n",
      "1           1  5rQSQlZXXjMcevPGoAfE1z  Salastkbron   \n",
      "2           2  4wS0TnQzVkY9ML1BPKpOk1    Tiago PZK   \n",
      "3           3  7bywjHOc0wSjGGbj04XbVi         Feid   \n",
      "4           4  6XjDF6nds4DE2BBbagZol6   FloyyMenor   \n",
      "\n",
      "                                          Track_Name  Popularity Explicit  \\\n",
      "0  BESAME (feat. Tiago PZK, Khea & Neo Pistea) - ...          86        1   \n",
      "1                                      Un Besito Más          79        0   \n",
      "2                                               Piel          86        1   \n",
      "3                                               LUNA          93        0   \n",
      "4                                          Gata Only          98        1   \n",
      "\n",
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.728   0.618    9    -6.621     1       0.0673         0.203   \n",
      "1         0.693   0.641    2    -5.775     0       0.0359         0.368   \n",
      "2         0.646   0.907    0    -1.617     1       0.0593         0.422   \n",
      "3         0.774   0.860    7    -2.888     0       0.1300         0.131   \n",
      "4         0.791   0.499    8    -8.472     0       0.0509         0.446   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  \n",
      "0          0.000000    0.1270    0.217  130.104  \n",
      "1          0.000000    0.2940    0.514   92.010  \n",
      "2          0.000000    0.1400    0.816  168.004  \n",
      "3          0.000000    0.1160    0.446  100.019  \n",
      "4          0.000024    0.0899    0.669   99.986  \n",
      "\n",
      "\n",
      "Datenvorverarbeitung...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '0mJRX0WnqWlu8Boe7gpQ1P'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d8/gs1syxf55h36dfnt2jysrf680000gn/T/ipykernel_60291/1626398021.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Skalierung der Daten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Modell erstellen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             return_tuple = (\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \"\"\"\n\u001b[1;32m    874\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1471\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \"\"\"\n\u001b[1;32m    911\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    913\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    994\u001b[0m                         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m                 raise ValueError(\n\u001b[1;32m   1000\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads-spotify/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     def __array__(\n\u001b[1;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '0mJRX0WnqWlu8Boe7gpQ1P'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sensible Daten aus Umgebungsvariablen lesen\n",
    "db_user = os.getenv('DB_USER', 'root')\n",
    "db_password = os.getenv('DB_PASSWORD', 'example')\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_name = os.getenv('DB_NAME', 'spotify_data')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "# Inspector verwenden, um Tabelleninformationen abzurufen\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Alle Tabellennamen abrufen\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Funktion zur Modellbewertung und Visualisierung\n",
    "def evaluate_model(model, X_test_scaled, y_test, y_pred_proba, history):\n",
    "    y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Berechnung der Modellgütemaße\n",
    "    mse = mean_squared_error(y_test, y_pred_proba)\n",
    "    r2 = r2_score(y_test, y_pred_proba)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "\n",
    "    # Zusätzliche Modellgütemaße für Klassifikation\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # ROC und AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "    # Visualisierung der Trainings- und Validierungsverluste\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualisierung der tatsächlichen vs. vorhergesagten Werte\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(y_test, y_pred_proba, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Probabilities')\n",
    "    plt.title('True vs Predicted Probabilities')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Kurve visualisieren\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Durchlaufen aller Tabellen\n",
    "for table_name in table_names:\n",
    "    print(f\"\\nAnalysiere Tabelle: {table_name}\")\n",
    "    query = f\"SELECT * FROM `{table_name}`\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    if df.shape[1] < 2:\n",
    "        print(f\"Nicht genug Daten in Tabelle '{table_name}' für ML-Analyse.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Erster Blick auf die Daten in Tabelle '{table_name}':\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Datenaufbereitung\n",
    "    print(\"Datenvorverarbeitung...\")\n",
    "\n",
    "    # Annahme: Die letzte Spalte ist das Ziel (Target) und die restlichen Spalten sind Features\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    # Train-Test-Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Skalierung der Daten\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Modell erstellen\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Sigmoid-Aktivierung für binäre Klassifikation\n",
    "    ])\n",
    "\n",
    "    # Modell kompilieren\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Modelltraining mit Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Modellbewertung\n",
    "    print(\"Modellbewertung...\")\n",
    "    y_pred_proba = model.predict(X_test_scaled)\n",
    "    evaluate_model(model, X_test_scaled, y_test, y_pred_proba, history)\n",
    "\n",
    "print(\"Modellinterpretation und Evaluation abgeschlossen.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads-spotify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
