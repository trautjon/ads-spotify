{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 1: Daten in die SQL-Datenbank importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erforderliche Pakete installieren\n",
    "pip install pandas sqlalchemy pymysql openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Float\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Sensible Daten aus Umgebungsvariablen lesen\n",
    "db_user = os.getenv('DB_USER', 'root')\n",
    "db_password = os.getenv('DB_PASSWORD', 'example')\n",
    "db_host = os.getenv('DB_HOST', 'localhost')\n",
    "db_name = os.getenv('DB_NAME', 'spotify_data')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "# Ordnerpfad mit Excel-Dateien (entweder als rohe Zeichenfolge oder mit doppelten Backslashes)\n",
    "folder_path = r'C:\\applied_data_science\\gruppen_projekt\\ads-spotify\\ads-spotify\\2024-04-27'\n",
    "# oder\n",
    "# folder_path = 'C:\\\\applied_data_science\\\\gruppen_projekt\\\\ads-spotify\\\\ads-spotify\\\\2024-04-27'\n",
    "\n",
    "# Funktion, um SQLAlchemy-Datentypen basierend auf Pandas-Datentypen zu bestimmen\n",
    "def map_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return Integer()\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return Float()\n",
    "    else:\n",
    "        return String(255)  # Standard-Stringlänge 255\n",
    "\n",
    "# Alle Excel-Dateien im Ordner durchgehen\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Excel-Datei in ein DataFrame laden\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Tabellenname basierend auf dem Dateinamen (ohne Erweiterung)\n",
    "        table_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Metadaten und Tabelle definieren\n",
    "        metadata = MetaData()\n",
    "        columns = []\n",
    "        for col_name, dtype in zip(df.columns, df.dtypes):\n",
    "            col_type = map_dtype(dtype)\n",
    "            columns.append(Column(col_name, col_type))\n",
    "        \n",
    "        table = Table(table_name, metadata, *columns)\n",
    "        \n",
    "        try:\n",
    "            # Tabelle in der Datenbank erstellen\n",
    "            metadata.create_all(engine)\n",
    "            \n",
    "            # Daten in die Tabelle einfügen\n",
    "            df.to_sql(table_name, engine, index=False, if_exists='append')\n",
    "            print(f\"Tabelle '{table_name}' erfolgreich erstellt und Daten importiert.\")\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Fehler beim Erstellen der Tabelle '{table_name}': {e}\")\n",
    "\n",
    "print(\"Vorgang abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2: Explorative Datenanalyse (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA_Script.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine('mysql+pymysql://zilancavas:zilanc33@localhost:3306/SpotifyCharts')\n",
    "\n",
    "# Daten aus der Datenbank laden\n",
    "query = \"SELECT * FROM table_name\"  # Ersetzen Sie 'table_name' durch den tatsächlichen Tabellennamen\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Ersten Überblick über die Daten bekommen\n",
    "print(\"Erster Blick auf die Daten:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nStatistische Zusammenfassung der numerischen Daten:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nInformationen über das DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Fehlende Werte analysieren\n",
    "print(\"\\nAnzahl fehlender Werte pro Spalte:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Verteilung von numerischen Variablen\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(\"\\nVerteilung der numerischen Variablen:\")\n",
    "df[numerical_columns].hist(bins=15, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Korrelationen zwischen numerischen Variablen\n",
    "print(\"\\nKorrelationsmatrix der numerischen Variablen:\")\n",
    "corr_matrix = df[numerical_columns].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Korrelationsmatrix der numerischen Variablen\")\n",
    "plt.show()\n",
    "\n",
    "# Verteilung von kategorialen Variablen\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"\\nVerteilung der kategorialen Variablen:\")\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(data=df, x=column, palette='Set2')\n",
    "    plt.title(f\"Verteilung der Kategorie: {column}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Paarweise Verteilung von ausgewählten Variablen\n",
    "print(\"\\nPaarweise Verteilung von ausgewählten Variablen:\")\n",
    "selected_columns = numerical_columns[:5]  # Wählen Sie bis zu 5 Spalten für die Paarplot\n",
    "sns.pairplot(df[selected_columns])\n",
    "plt.show()\n",
    "\n",
    "# Boxplots zur Analyse von Ausreissern\n",
    "print(\"\\nBoxplots zur Analyse von Ausreissern:\")\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(data=df, x=column, palette='Set2')\n",
    "    plt.title(f\"Boxplot der Variable: {column}\")\n",
    "    plt.show()\n",
    "\n",
    "# Analyse von Beziehungen zwischen kategorialen und numerischen Variablen\n",
    "print(\"\\nAnalyse von Beziehungen zwischen kategorialen und numerischen Variablen:\")\n",
    "for cat_col in categorical_columns:\n",
    "    for num_col in numerical_columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.boxplot(data=df, x=cat_col, y=num_col, palette='Set2')\n",
    "        plt.title(f\"Beziehung zwischen {cat_col} und {num_col}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "# Zeitanalyse, falls Datumsspalte vorhanden ist\n",
    "date_columns = df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "\n",
    "print(\"\\nZeitanalyse, falls Datumsspalte vorhanden ist:\")\n",
    "for date_col in date_columns:\n",
    "    df[date_col] = pd.to_datetime(df[date_col])  # Konvertieren zu datetime\n",
    "    df.set_index(date_col, inplace=True)\n",
    "    df.resample('M').mean().plot(figsize=(15, 5))\n",
    "    plt.title(f\"Zeitanalyse der Spalte: {date_col}\")\n",
    "    plt.show()\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "print(\"\\nExplorative Datenanalyse abgeschlossen.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 3: Machine Learning Modelle erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML_Script.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine('mysql+pymysql://zilancavas:zilanc33@localhost:3306/SpotifyCharts')\n",
    "\n",
    "# Daten aus der Datenbank laden\n",
    "query = \"SELECT * FROM table_name\"  # Ersetzen Sie 'table_name' durch den tatsächlichen Tabellennamen\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Datenaufbereitung\n",
    "print(\"Datenvorverarbeitung...\")\n",
    "\n",
    "# Beispiel: Feature und Target definieren\n",
    "# Ersetzen Sie 'Feature1', 'Feature2', ... und 'Target' durch die tatsächlichen Spaltennamen\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = df['Target']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numerische und kategoriale Spalten identifizieren\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing Pipelines für numerische und kategoriale Daten\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Modellauswahl und -training\n",
    "\n",
    "# Linear Regression\n",
    "print(\"Training des linearen Regressionsmodells...\")\n",
    "lr_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Random Forest Regressor\n",
    "print(\"Training des Random Forest Regressors...\")\n",
    "rf_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter-Tuning mit GridSearchCV\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Modellbewertung\n",
    "print(\"Modellbewertung...\")\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - Mean Squared Error: {mse}\")\n",
    "    print(f\"{model_name} - R² Score: {r2}\")\n",
    "\n",
    "# Bewertung der Modelle\n",
    "evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest Regressor\")\n",
    "\n",
    "# Visualisierung der Ergebnisse\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred_lr, alpha=0.5, label='Linear Regression')\n",
    "plt.scatter(y_test, y_pred_rf, alpha=0.5, label='Random Forest Regressor')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print(\"Machine Learning Analyse abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 4: Geographische Datenintegration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erforderliche Pakete installieren:\n",
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographical_Data_Script.ipynb\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine('mysql+pymysql://zilancavas:zilanc33@localhost:3306/SpotifyCharts')\n",
    "\n",
    "# Beispielhafte Daten laden (ersetzen Sie 'table_name' und 'geo_column' durch Ihre tatsächlichen Namen)\n",
    "query = \"SELECT * FROM table_name\"  # Ersetzen Sie 'table_name' durch den tatsächlichen Tabellennamen\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Beispielhafte geographische Daten laden\n",
    "# Hier verwenden wir GeoJSON-Dateien, die gängige geographische Datenformate sind\n",
    "gdf = gpd.read_file('path/to/your/geojson/file.geojson')  # Ersetzen Sie den Pfad durch den tatsächlichen Pfad\n",
    "\n",
    "# Daten zusammenführen\n",
    "# Angenommen, df hat eine Spalte 'location_id', die mit einer Spalte 'id' in gdf übereinstimmt\n",
    "merged = gdf.merge(df, left_on='id', right_on='location_id')\n",
    "\n",
    "# Geographische Daten anzeigen\n",
    "print(\"Geographische Daten:\")\n",
    "print(gdf.head())\n",
    "\n",
    "# Datenvisualisierung\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "gdf.plot(ax=ax, color='blue', edgecolor='black')\n",
    "plt.title(\"Geographische Datenvisualisierung\")\n",
    "plt.show()\n",
    "\n",
    "# Detaillierte Visualisierung mit Daten\n",
    "# Beispiel: Visualisierung der Datenverteilung auf der Karte\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "merged.plot(column='data_column', ax=ax, legend=True,\n",
    "            legend_kwds={'label': \"Data Value\",\n",
    "                         'orientation': \"horizontal\"})\n",
    "plt.title(\"Geographische Daten mit Attributen\")\n",
    "plt.show()\n",
    "\n",
    "# Weitere Analysen\n",
    "# Beispiel: Durchschnittswert pro geographische Einheit berechnen\n",
    "average_values = merged.groupby('geo_column')['data_column'].mean().reset_index()\n",
    "print(\"Durchschnittswerte pro geographische Einheit:\")\n",
    "print(average_values)\n",
    "\n",
    "# Visualisierung der Durchschnittswerte auf der Karte\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "merged_avg = gdf.merge(average_values, left_on='id', right_on='geo_column')\n",
    "merged_avg.plot(column='data_column', ax=ax, legend=True,\n",
    "                legend_kwds={'label': \"Average Value\",\n",
    "                             'orientation': \"horizontal\"})\n",
    "plt.title(\"Durchschnittswerte pro geographische Einheit\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Geographische Datenanalyse abgeschlossen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 5: Fortgeschrittene Deep Learning Techniken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erforderliches Pakete installieren:\n",
    "pip install tensorflow pandas sqlalchemy pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Sicherstellen, dass die NLTK-Ressourcen heruntergeladen sind\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine('mysql+pymysql://zilancavas:zilanc33@localhost:3306/SpotifyCharts')\n",
    "\n",
    "# Daten aus der Datenbank laden\n",
    "query = \"SELECT * FROM table_name\"  # Ersetzen Sie 'table_name' durch den tatsächlichen Tabellennamen\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Datenvorverarbeitung\n",
    "print(\"Datenvorverarbeitung...\")\n",
    "\n",
    "# Beispiel: Feature und Target definieren\n",
    "# Ersetzen Sie 'Feature1', 'Feature2', ... und 'Target' durch die tatsächlichen Spaltennamen\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = df['Target']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Skalierung der Daten\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modell erstellen - MLP\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Regression - keine Aktivierungsfunktion in der Ausgangsschicht\n",
    "])\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Modelltraining mit Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Modellbewertung - MLP\n",
    "print(\"Modellbewertung - MLP...\")\n",
    "\n",
    "# Vorhersagen\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Berechnung der Modellgütemasse\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Trainings- und Validierungsverluste visualisieren\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Visualisierung der tatsächlichen vs. vorhergesagten Werte\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "print(\"Deep Learning Analyse abgeschlossen.\")\n",
    "\n",
    "# Ergänzung: Verwendung eines Convolutional Neural Networks (CNN)\n",
    "\n",
    "# Reshape der Daten für CNN\n",
    "X_train_cnn = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1, 1)\n",
    "X_test_cnn = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1, 1)\n",
    "\n",
    "# Modell erstellen - CNN\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(X_train_cnn.shape[1], 1, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Regression - keine Aktivierungsfunktion in der Ausgangsschicht\n",
    "])\n",
    "\n",
    "# Modell kompilieren - CNN\n",
    "cnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Modelltraining mit Early Stopping - CNN\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Modellbewertung - CNN\n",
    "print(\"Modellbewertung - CNN...\")\n",
    "\n",
    "# Vorhersagen - CNN\n",
    "y_pred_cnn = cnn_model.predict(X_test_cnn)\n",
    "\n",
    "# Berechnung der Modellgütemasse - CNN\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
    "print(f\"Mean Squared Error - CNN: {mse_cnn}\")\n",
    "print(f\"R² Score - CNN: {r2_cnn}\")\n",
    "\n",
    "# Trainings- und Validierungsverluste visualisieren - CNN\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_cnn.history['loss'], label='Training Loss - CNN')\n",
    "plt.plot(history_cnn.history['val_loss'], label='Validation Loss - CNN')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss - CNN')\n",
    "plt.show()\n",
    "\n",
    "# Visualisierung der tatsächlichen vs. vorhergesagten Werte - CNN\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred_cnn, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted Values - CNN')\n",
    "plt.show()\n",
    "\n",
    "print(\"Deep Learning Analyse - CNN abgeschlossen.\")\n",
    "\n",
    "# Ergänzung: Verwendung von NLP-Techniken (z.B. Sentiment Analysis)\n",
    "\n",
    "# Sentiment Analysis mit VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Beispiel: Sentiment Analysis für einen Text\n",
    "example_text = \"This is a great example of using NLP techniques.\"\n",
    "sentiment = sia.polarity_scores(example_text)\n",
    "print(f\"Sentiment Analysis Ergebnis: {sentiment}\")\n",
    "\n",
    "# Optional: Sentiment Analysis auf Textdaten in der Datenbank anwenden\n",
    "# Angenommen, es gibt eine Spalte 'TextColumn' in der Datenbank\n",
    "if 'TextColumn' in df.columns:\n",
    "    df['sentiment'] = df['TextColumn'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    print(\"Sentiment Analysis auf Textdaten angewendet.\")\n",
    "    print(df[['TextColumn', 'sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 6: Modellinterpretation und Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erforderliche Pakete installieren:\n",
    "pip install tensorflow pandas sqlalchemy pymysql scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_Evaluation_Script.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verbindung zur MySQL-Datenbank herstellen\n",
    "engine = create_engine('mysql+pymysql://zilancavas:zilanc33@localhost:3306/SpotifyCharts')\n",
    "\n",
    "# Daten aus der Datenbank laden\n",
    "query = \"SELECT * FROM table_name\"  # Ersetzen Sie 'table_name' durch den tatsächlichen Tabellennamen\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Datenaufbereitung\n",
    "print(\"Datenvorverarbeitung...\")\n",
    "\n",
    "# Beispiel: Feature und Target definieren\n",
    "# Ersetzen Sie 'Feature1', 'Feature2', ... und 'Target' durch die tatsächlichen Spaltennamen\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = df['Target']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Skalierung der Daten\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modell erstellen\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid-Aktivierung für binäre Klassifikation\n",
    "])\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Modelltraining mit Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Modellbewertung\n",
    "print(\"Modellbewertung...\")\n",
    "\n",
    "# Vorhersagen\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "# Berechnung der Modellgütemaße\n",
    "mse = mean_squared_error(y_test, y_pred_proba)\n",
    "r2 = r2_score(y_test, y_pred_proba)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Zusätzliche Modellgütemaße für Klassifikation\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ROC und AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "# Visualisierung der Trainings- und Validierungsverluste\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Visualisierung der tatsächlichen vs. vorhergesagten Werte\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred_proba, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Probabilities')\n",
    "plt.title('True vs Predicted Probabilities')\n",
    "plt.show()\n",
    "\n",
    "# ROC Kurve visualisieren\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(\"Modellinterpretation und Evaluation abgeschlossen.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads-spotify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
